---
title: "Part 2"
author: "Pedro Lins"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
############
# Packages #
############
library(plyr)
library(gapminder)
library(tidyverse)
library(knitr)
library(ggplot2)
library(lmtest) # tests
library(MASS) # boxcox
library(nortest) #ad.test for resifual normality 
library(PerformanceAnalytics) # correlation map
library(car) #variance factorinflation factor
library(caret) #model cross validation
```

```{r}
#############
# Load Data #
#############
openpowerlifting = "openpowerlifting.csv" 
df = read_csv(openpowerlifting)
```


```{r}
#####################
# Checking the data #
#####################
#understand the quirks of the data set and potential errors

head(df) #looking at data
summary(df) #checking data

str(df)
sapply(lapply(df, unique), length)
```


```{r}
#############################
# Exploratory Data Analysis #
#############################
#to understand properties of the data
#to inspect qualitative features rather than a huge table of raw data
#to discover new patterns or associations

#quantitative variables: Age,Bodyweight, Squat (1,2,3), Bench (1,2,3), Deadlift(1,2,3), Total,  Dots, Wilks Glossbrenner, Goodlift, Date, Year


ggplot(df,aes(x = Squat1Kg, y= TotalKg, color = Equipment))+
  geom_point(alpha = 0.6) +
  theme_minimal()+
  ggtitle("Scatterplot: Squat x TotalKg")

ggplot(df,aes(x = Sex, y = Wilks, fill = Sex)) +
  geom_violin(alpha = 0.6) +
  theme_minimal() + 
  ggtitle("Violin: Wilks")

ggplot(df,aes(x = Sex, y = Wilks, fill = Sex)) +
  geom_boxplot(alpha = 0.6) +
  theme_minimal() + 
  ggtitle("Boxplot: TotalKg")
```


```{r}
######################################
# Statistical modeling and inference #
######################################

# I did it because date was in DD/MM/YYYY format, to make it easier for analyse, I made it just YYYY. 
df$Year = c(format(df$Date, "%Y"))
df$Year = as.numeric(df$Year)

df2 = df %>%
  filter(MeetName == "World Classic Powerlifting Championships") %>%
  filter(Squat1Kg > 0) %>%
  filter(Bench1Kg > 0) %>%
  filter(Deadlift1Kg > 0) %>%
  filter(Year == 2019) 

summary(df2) # looking for NAs
str(df2) # looking for NAs
df2=df2[-c(14,19,24,34,39)] #drop NAs  columns
df2 = na.omit(df2) #drop NAs rows
sapply(lapply(df2, unique), length)
df2=df2[-c(1,3,4,6,7,8,10,12:14,16:18,20:22,24:37)] #drop single values variable
sapply(lapply(df2, unique), length)

dfcor = data.frame(df2$Bench1Kg, df2$Squat1Kg, df2$Deadlift1Kg, df2$Age, df2$BodyweightKg)
chart.Correlation(dfcor, histogram=TRUE, pch=19) #has to be lower than 70%

mod = lm(TotalKg ~ ., df2) #first try
summary(mod)

mod = lm(TotalKg ~ . - Age, df2) #removing Age
summary(mod)

mod = lm(TotalKg ~ . - Sex - Age, df2) #removing Age
summary(mod)

mod = lm(TotalKg ~ BodyweightKg + Squat1Kg + Bench1Kg + Deadlift1Kg, df2) #second try
summary(mod)

mod = lm(TotalKg ~ Bench1Kg + Deadlift1Kg, df2)
mod = lm(TotalKg ~ Bench1Kg + Squat1Kg, df2)
summary(mod)

#collinearity
vif = vif(mod)  #should be lower than 5
barplot(vif, main = "VIF Values" ,col = "blue")
abline( h = 5, col = "red") #remove squat since it has the higest VIF

mod = lm(TotalKg ~ . - Squat1Kg, df2) #removing first squat
summary(mod)

mod = lm(TotalKg ~ . - Squat1Kg - Sex, df2) #removing first squat
summary(mod)

#collinearity
vif = vif(mod)  #should be lower than 5
barplot(vif, main = "VIF Values" ,col = "blue") #without Squat1Kg
abline( h = 5, col = "red") 


#####################
# Residual analysis #
#####################
anares = rstandard(mod)

# normality test for residues
ad.test(anares) # normal if greater than 0.05
shapiro.test(anares) #normal if greater than 0.05

# homoscedasticity test
bptest(mod) #fail to reject the null hypothesis (p-value > 0.05)

# autocorrelation test
dwtest(mod) #should be between 1.50 and 2.50

# define plotting area
par(mfrow=c(2,2))

# plot residual analysis
plot(aov(mod)) 

df2 = df2[-45,] #removing outlier

mod = lm(TotalKg ~ . - Squat1Kg - Sex, df2) #removing first squat
summary(mod)

anares = rstandard(mod)

# normality test for residues
ad.test(anares) # normal if greater than 0.05
shapiro.test(anares) #normal if greater than 0.05

# homoscedasticity test
bptest(mod) #fail to reject the null hypothesis (p-value > 0.05)

# autocorrelation test
dwtest(mod) #should be between 1.50 and 2.50

# define plotting area
par(mfrow=c(2,2))

# plot residual analysis
plot(aov(mod)) 


```

```{r}
##############
# Best model #
##############

df2 = df %>%
  filter(MeetName == "World Classic Powerlifting Championships") %>%
  filter(Squat1Kg > 0) %>%
  filter(Bench1Kg > 0) %>%
  filter(Deadlift1Kg > 0) %>%
  filter(Year == 2019)  %>%
  filter(Sex == "F") 

# For men, it only works with those subsets:
  #filter(WeightClassKg == "59" | WeightClassKg == "66" | WeightClassKg == "74"| WeightClassKg == "93"| WeightClassKg == "53")
  #filter( WeightClassKg == "105" | WeightClassKg == "120+")
  #filter(WeightClassKg == "83" | WeightClassKg == "120")

summary(df2) # looking for NAs
str(df2) # looking for NAs
df2=df2[-c(14,19,24,34,39)] #drop NAs  columns
df2 = na.omit(df2) #drop NAs rows
sapply(lapply(df2, unique), length)
df2=df2[-c(1,2,3,4,6,7,8,10,12:14,16:18,20:22,24:37)] #drop single values variable
sapply(lapply(df2, unique), length)

dfcor = data.frame(df2$Bench1Kg, df2$Squat1Kg, df2$Deadlift1Kg, df2$Age, df2$BodyweightKg)
chart.Correlation(dfcor, histogram=TRUE, pch=19) #should to be lower than 70%

mod = lm(TotalKg ~ ., df2) #first try
summary(mod)

vif(mod) #vif is not high even with all three lifts

anares = rstandard(mod)

# normality test for residues
ad.test(anares) # normal if greater than 0.05
shapiro.test(anares) #normal if greater than 0.05

# homoscedasticity test
bptest(mod) #fail to reject the null hypothesis (p-value > 0.05)

# autocorrelation test
dwtest(mod) #should be between 1.50 and 2.50

# define plotting area
par(mfrow=c(2,2))

# plot residual analysis
plot(aov(mod))

df2=df2[-c(190,302),] #female outlier

mod = lm(TotalKg ~ ., df2) #first try
summary(mod)

#collinearity
vif = vif(mod)  #should be lower than 5
barplot(vif, main = "VIF Values" ,col = "blue") #without Squat1Kg
abline( h = 5, col = "red") 

anares = rstandard(mod)

# normality test for residues
ad.test(anares) # normal if greater than 0.05
shapiro.test(anares) #normal if greater than 0.05

# homoscedasticity test
bptest(mod) #fail to reject the null hypothesis (p-value > 0.05)

# autocorrelation test
dwtest(mod) #should be between 1.50 and 2.50

# define plotting area
par(mfrow=c(2,2))

# plot residual analysis
plot(aov(mod))
```


```{r}
#############################
# Model performance metrics #
#############################

#########################
# LOOCV - Leave one out #
#########################
#this method has higher execution time because it is repeated for as many data sets we have

# Define training control
train.control <- trainControl(method = "LOOCV")
# Train the model
model <- train(TotalKg ~., data = df2, method = "lm",
               trControl = train.control)
# Summarize the results
print(model)

# Define training control
train.control <- trainControl(method = "LOOCV")
# Train the model
model2 <- train(TotalKg ~ Age + Squat1Kg + Bench1Kg + Deadlift1Kg, data = df2, method = "lm",
               trControl = train.control)
# Summarize the results
print(model2)

mod = lm(TotalKg ~ Age + Squat1Kg + Bench1Kg + Deadlift1Kg, df2)
#collinearity
vif = vif(mod)  #should be lower than 5
barplot(vif, main = "VIF Values" ,col = "blue") #without Squat1Kg
abline( h = 5, col = "red") 
```

```{r}
###################################
# Prediction and machine learning #
###################################

#Commonwealth Womens Classic Powerlifting Championships
dfnew = read_csv("nominationipf.csv")
pred = predict(mod, dfnew, interval="confidence") #confidence
predictions=data.frame(dfnew$Name,dfnew$Total,pred)
predictions$fit = round(predictions$fit)
predictions$lwr = round(predictions$lwr)
predictions$upr = round(predictions$upr)
colnames(predictions) = c("Name","Total","Prediction", "Lower", "Upper")
write.csv(predictions,"CommonwealthPredictions.csv", row.names = TRUE)

```

```{r}
#########################
# Prediction Assessment #
#########################

dfpred = read_csv("CommonwealthPredictions.csv")
dfpred$Erro = (1-(dfpred$Total/dfpred$Prediction))*100 #error in percentedge
summary(dfpred)

#Visualizing results
ggplot(dfpred, aes(x=Total, y=Prediction))+
  geom_point()+
  geom_smooth()+
  theme_minimal()

ggplot(dfpred, aes(x=Total, y=Erro))+
  geom_point()+
  geom_smooth()+
  theme_minimal()

#Error analysis
ggplot(dfpred, aes(Erro))+
  geom_histogram(aes(y=..density..), colour="black", fill="blue", alpha = 0.6)+
  geom_density(alpha=.2, fill="black")+
  geom_vline(aes(xintercept=mean(Erro)),
            color="red", linetype="dashed", size=1)+
  theme_minimal()

# normality test for error
ad.test(dfpred$Erro) # normal if greater than 0.05
shapiro.test(dfpred$Erro) #normal if greater than 0.05

#Removing outliers
dfpred = dfpred[-c(54,76,78),]

# normality test for error
ad.test(dfpred$Erro) # normal if greater than 0.05
shapiro.test(dfpred$Erro) #normal if greater than 0.05

summary(dfpred)

```

